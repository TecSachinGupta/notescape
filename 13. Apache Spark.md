# Apache Spark Theory Guide

## Architecture

Apache Spark follows a master-slave architecture with the following key components:

### Driver Program
The driver program runs the main function and creates the SparkContext. It's responsible for converting user programs into tasks, scheduling tasks on executors, and coordinating the overall execution. The driver maintains information about the application and responds to user's program or input.

### Cluster Manager
The cluster manager is responsible for acquiring resources across the cluster. Spark supports several cluster managers including Standalone, Apache Mesos, Hadoop YARN, and Kubernetes. The cluster manager allocates resources and launches executor processes.

### Executors
Executors are worker processes that run on cluster nodes. They execute tasks assigned by the driver, store data for the application in memory or disk storage, and return results to the driver. Each executor has multiple cores and can run multiple tasks in parallel.

### SparkContext
SparkContext is the entry point for all Spark functionality. It coordinates the execution of Spark applications and manages the connection to the cluster. Only one SparkContext can be active per JVM.

## DAG (Directed Acyclic Graph)

Spark creates a DAG of stages for each job. The DAG scheduler divides the logical execution plan into stages based on wide transformations (shuffles). Each stage contains a sequence of narrow transformations that can be pipelined together.

### Stage Types
- **Shuffle Map Stage**: Produces intermediate output for shuffles
- **Result Stage**: Computes the final result of an action

### Task Scheduling
The DAG scheduler submits stages to the task scheduler, which launches tasks on executors. Tasks within a stage can run in parallel, while stages have dependencies that must be respected.

## RDD vs DataFrame vs Dataset

### RDD (Resilient Distributed Dataset)
RDDs are the fundamental data structure of Spark, representing an immutable distributed collection of objects. They provide fault tolerance through lineage information and support both transformations and actions.

**Key Characteristics:**
- Immutable and distributed
- Fault-tolerant through lineage
- Lazy evaluation
- Type-safe at compile time
- No built-in optimization

**Use Cases:**
- Low-level transformations and actions
- Manipulating unstructured data
- Complex operations requiring fine-grained control

### DataFrame
DataFrames are distributed collections of data organized into named columns, similar to tables in relational databases. They're built on top of RDDs but provide a higher-level API with automatic optimization.

**Key Characteristics:**
- Schema-based structure
- Catalyst optimizer integration
- Language-agnostic API
- Runtime type safety
- Automatic optimization

**Use Cases:**
- Structured and semi-structured data
- SQL-like operations
- Data analysis and reporting

### Dataset
Datasets combine the benefits of RDDs (compile-time type safety) with DataFrames (Catalyst optimizer). They provide a type-safe, object-oriented programming interface.

**Key Characteristics:**
- Compile-time type safety
- Catalyst optimizer benefits
- Object-oriented programming interface
- Encoder-based serialization
- Unified API for batch and streaming

**Use Cases:**
- Type-safe operations on structured data
- Complex transformations requiring custom objects
- Applications requiring both performance and type safety

## Broadcast Variables

Broadcast variables allow efficient sharing of read-only data across all nodes in a cluster. Instead of shipping a copy of the variable with each task, Spark distributes the variable once per executor using efficient broadcast algorithms.

### Benefits
- Reduces network traffic and memory usage
- Improves performance for large lookup tables
- Avoids serialization overhead for each task

### Usage Example
```scala
val broadcastVar = spark.sparkContext.broadcast(Map("key1" -> "value1"))
// Use in transformations
rdd.map(x => broadcastVar.value.get(x))
```

### Best Practices
- Use for read-only data shared across multiple tasks
- Ideal for lookup tables, configuration parameters
- Unpersist when no longer needed to free memory

## Partitioning

Partitioning determines how data is distributed across the cluster. Proper partitioning is crucial for performance, as it affects parallelism, data locality, and shuffle operations.

### Default Partitioning
- Input partitions typically match HDFS blocks
- Default parallelism set by `spark.default.parallelism`
- Generally 2-3 partitions per CPU core

### Custom Partitioning
- **Hash Partitioning**: Uses hash function on keys
- **Range Partitioning**: Distributes data based on key ranges
- **Custom Partitioning**: User-defined partitioning logic

### Considerations
- Too few partitions: Underutilized resources, large tasks
- Too many partitions: Scheduling overhead, small tasks
- Optimal size: 100MB-1GB per partition

## Repartition vs Coalesce

Both operations change the number of partitions but work differently:

### Repartition
- Performs a full shuffle of data
- Can increase or decrease partitions
- Distributes data evenly across partitions
- More expensive due to shuffle operation
- Use when you need even distribution

### Coalesce
- Minimizes data movement
- Can only decrease partitions
- May result in uneven partition sizes
- More efficient for reducing partitions
- Use when reducing partitions after filtering

### Decision Guidelines
- Use `coalesce` when reducing partitions after filtering large datasets
- Use `repartition` when you need even distribution or increasing partitions
- Consider data skew when choosing between them

## Optimization Techniques

### Catalyst Optimizer
The Catalyst optimizer automatically optimizes DataFrame and Dataset operations through rule-based and cost-based optimization.

#### Rule-Based Optimization
- Predicate pushdown: Filters moved closer to data sources
- Projection pushdown: Only required columns are read
- Constant folding: Compile-time evaluation of constants
- Boolean expression simplification

#### Cost-Based Optimization
- Join reordering based on table sizes
- Optimal join strategy selection
- Statistics-based decision making

### Performance Tuning
- Use appropriate file formats (Parquet, Delta)
- Implement columnar storage for analytical workloads
- Optimize serialization with Kryo
- Tune garbage collection settings
- Configure appropriate resource allocation

## Caching and Persistence

Caching stores RDDs, DataFrames, or Datasets in memory or disk for faster access in subsequent actions.

### Storage Levels
- **MEMORY_ONLY**: Store in memory as deserialized objects
- **MEMORY_AND_DISK**: Store in memory, spill to disk if needed
- **MEMORY_ONLY_SER**: Store in memory as serialized objects
- **DISK_ONLY**: Store only on disk
- **MEMORY_AND_DISK_SER**: Serialized in memory, spill to disk

### Cache vs Persist
- `cache()`: Shorthand for `persist(MEMORY_ONLY)`
- `persist()`: Allows specifying storage level
- `unpersist()`: Manually removes cached data

### Best Practices
- Cache datasets used multiple times
- Use serialized storage for memory-constrained environments
- Monitor cache usage through Spark UI
- Unpersist unused cached data

## Spark-Submit Configuration

Spark-submit is the command-line tool for submitting Spark applications to clusters.

### Essential Configurations
```bash
spark-submit \
  --class MainClass \
  --master yarn \
  --deploy-mode cluster \
  --driver-memory 4g \
  --executor-memory 8g \
  --executor-cores 4 \
  --num-executors 10 \
  --conf spark.sql.adaptive.enabled=true \
  application.jar
```

### Key Parameters
- **--master**: Cluster manager (local, yarn, mesos, k8s)
- **--deploy-mode**: Driver location (client, cluster)
- **--driver-memory**: Memory for driver process
- **--executor-memory**: Memory per executor
- **--executor-cores**: CPU cores per executor
- **--num-executors**: Number of executors (YARN/Mesos)

### Dynamic Allocation
- `spark.dynamicAllocation.enabled=true`
- `spark.dynamicAllocation.minExecutors=1`
- `spark.dynamicAllocation.maxExecutors=20`
- `spark.dynamicAllocation.initialExecutors=5`

## Join Operations

### Broadcast Hash Join
Used when one dataset is small enough to fit in memory across all executors.

**Characteristics:**
- No shuffle required
- Fastest join type
- Automatic when table size < `spark.sql.autoBroadcastJoinThreshold`
- Memory-intensive for large broadcast tables

**Best For:**
- Joining large table with small lookup table
- Dimension tables in star schema

### Sort-Merge Join
Default join strategy for large datasets that require shuffling.

**Characteristics:**
- Both datasets are shuffled and sorted
- Efficient for large datasets
- Requires shuffle operation
- Memory-efficient compared to hash joins

**Best For:**
- Large-to-large table joins
- When broadcast join isn't feasible

### Nested Loop Join
Fallback join when other strategies aren't applicable.

**Characteristics:**
- Cartesian product approach
- Most expensive join type
- Used for non-equi joins
- Should be avoided when possible

## Skew Handling

Data skew occurs when data is unevenly distributed across partitions, leading to performance bottlenecks.

### Identifying Skew
- Monitor task execution times in Spark UI
- Check partition sizes and record counts
- Look for straggler tasks in stages

### Mitigation Strategies

#### Salting Technique
Add random prefixes to keys to distribute skewed data:
```scala
// Add salt to skewed keys
val saltedDF = df.withColumn("salted_key", 
  concat(col("key"), lit("_"), (rand() * 100).cast("int"))
```

#### Adaptive Query Execution (AQE)
Enable AQE for automatic skew handling:
```scala
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.skewJoin.enabled", "true")
```

#### Custom Partitioning
Implement custom partitioners for better data distribution:
```scala
class CustomPartitioner(partitions: Int) extends Partitioner {
  def numPartitions: Int = partitions
  def getPartition(key: Any): Int = {
    // Custom partitioning logic
  }
}
```

#### Broadcast Join Optimization
Convert skewed joins to broadcast joins when possible:
```scala
spark.conf.set("spark.sql.autoBroadcastJoinThreshold", "200MB")
```

## Fault Tolerance

Spark provides fault tolerance through multiple mechanisms:

### RDD Lineage
RDDs maintain lineage information to recompute lost partitions. The lineage graph tracks transformations applied to create each RDD, enabling automatic recovery.

### Checkpointing
Checkpointing truncates lineage by saving RDD data to reliable storage:
```scala
spark.sparkContext.setCheckpointDir("hdfs://path/to/checkpoint")
rdd.checkpoint()
```

### Task-Level Fault Tolerance
- Failed tasks are automatically retried
- Configurable retry attempts: `spark.task.maxFailures`
- Tasks can be speculatively executed on slow nodes

### Driver Fault Tolerance
- Cluster mode: Driver runs on cluster nodes with restart capability
- Client mode: Driver failure requires application restart
- Kubernetes: Automatic driver pod restart

### Executor Fault Tolerance
- Lost executors are automatically replaced
- In-progress tasks are rescheduled
- Cached data is recomputed using lineage

### Best Practices
- Use checkpointing for long lineage chains
- Configure appropriate retry settings
- Monitor application health through Spark UI
- Implement proper error handling in user code
- Use cluster deploy mode for production applications

