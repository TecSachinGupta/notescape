# PySpark Theory Guide

## Table of Contents
1. [DataFrame Manipulations](#dataframe-manipulations)
2. [Window Functions](#window-functions)
3. [GroupBy Operations](#groupby-operations)
4. [Filtering](#filtering)
5. [Pivots and Unpivots](#pivots-and-unpivots)
6. [Column Renaming](#column-renaming)
7. [Anti and Semi Joins](#anti-and-semi-joins)
8. [Handling Large Files](#handling-large-files)
9. [Performance Tuning](#performance-tuning)
10. [Handling OOM Issues](#handling-oom-issues)

---

## DataFrame Manipulations

### Core Concepts
DataFrames in PySpark are distributed collections of structured data organized into named columns. They are conceptually equivalent to tables in relational databases but with richer optimizations under the hood.

### Key Operations

#### Creating DataFrames
```python
# From RDD
df = spark.createDataFrame(rdd, schema)

# From list of tuples
df = spark.createDataFrame([(1, "John"), (2, "Jane")], ["id", "name"])

# Reading from files
df = spark.read.option("header", "true").csv("file.csv")
```

#### Basic Transformations
- **select()**: Choose specific columns
- **withColumn()**: Add or modify columns
- **drop()**: Remove columns
- **filter()/where()**: Filter rows
- **distinct()**: Remove duplicates
- **orderBy()/sort()**: Sort data

#### Column Operations
```python
# Add new column
df.withColumn("new_col", col("existing_col") * 2)

# Conditional operations
df.withColumn("category", when(col("value") > 100, "high").otherwise("low"))

# String operations
df.withColumn("upper_name", upper(col("name")))
```

### Lazy Evaluation
PySpark uses lazy evaluation, meaning transformations are not executed until an action is called. This allows for query optimization through the Catalyst optimizer.

---

## Window Functions

### Theory
Window functions perform calculations across a set of rows that are related to the current row. Unlike regular aggregate functions, window functions don't collapse rows into a single output row.

### Window Specification
```python
from pyspark.sql.window import Window

# Basic window
window = Window.partitionBy("department").orderBy("salary")

# With frame specification
window = Window.partitionBy("department").orderBy("salary").rowsBetween(-1, 1)
```

### Types of Window Functions

#### Ranking Functions
- **row_number()**: Assigns unique sequential integers
- **rank()**: Assigns rank with gaps for ties
- **dense_rank()**: Assigns rank without gaps
- **percent_rank()**: Relative rank as percentage

#### Aggregate Functions
- **sum()**, **avg()**, **max()**, **min()**: Running aggregates
- **count()**: Running count

#### Analytic Functions
- **lag()**: Access previous row value
- **lead()**: Access next row value
- **first_value()**: First value in window
- **last_value()**: Last value in window

### Frame Specifications
- **ROWS**: Physical rows (unbounded/current/n preceding/following)
- **RANGE**: Logical range based on value differences
- **UNBOUNDED PRECEDING/FOLLOWING**: From start/end of partition

---

## GroupBy Operations

### Fundamental Concepts
GroupBy operations split data into groups based on one or more columns, apply a function to each group, and combine results.

### Basic GroupBy
```python
# Simple grouping
df.groupBy("department").count()

# Multiple columns
df.groupBy("department", "location").avg("salary")

# Multiple aggregations
df.groupBy("department").agg(
    avg("salary").alias("avg_salary"),
    max("salary").alias("max_salary"),
    count("*").alias("count")
)
```

### Advanced Aggregations
#### Custom Aggregation Functions
```python
from pyspark.sql.functions import collect_list, collect_set

df.groupBy("department").agg(
    collect_list("employee_name").alias("employees"),
    collect_set("skill").alias("unique_skills")
)
```

#### Pivot-like Operations with GroupBy
```python
df.groupBy("year").pivot("department").sum("revenue")
```

### GroupBy with Window Functions
Combine groupBy with window functions for complex analytics:
```python
df.withColumn("dept_avg", avg("salary").over(Window.partitionBy("department")))
```

---

## Filtering

### Basic Filtering
```python
# Simple conditions
df.filter(col("age") > 25)
df.where(col("salary") > 50000)

# Multiple conditions
df.filter((col("age") > 25) & (col("department") == "IT"))
```

### Advanced Filtering Techniques

#### String Filtering
```python
# Pattern matching
df.filter(col("name").rlike("^J.*"))
df.filter(col("email").contains("@company.com"))

# Case-insensitive
df.filter(upper(col("name")) == "JOHN")
```

#### Null Handling
```python
# Filter nulls
df.filter(col("column").isNull())
df.filter(col("column").isNotNull())

# Filter with null-safe equality
df.filter(col("column").eqNullSafe("value"))
```

#### List/Array Filtering
```python
# isin() for multiple values
df.filter(col("department").isin(["IT", "HR", "Finance"]))

# Array operations
df.filter(array_contains(col("skills"), "Python"))
```

### Performance Considerations
- **Predicate Pushdown**: Filters are pushed down to data sources when possible
- **Partition Pruning**: Filters on partition columns eliminate entire partitions
- **Column Pruning**: Only required columns are read

---

## Pivots and Unpivots

### Pivot Operations
Pivoting transforms rows into columns, creating a cross-tabulation.

#### Basic Pivot
```python
# Pivot with aggregation
df.groupBy("year").pivot("department").sum("revenue")

# Pivot with multiple values
df.groupBy("year").pivot("department", ["IT", "HR", "Finance"]).sum("revenue")
```

#### Advanced Pivot Techniques
```python
# Multiple aggregations in pivot
df.groupBy("year").pivot("department").agg(
    sum("revenue").alias("total_revenue"),
    avg("revenue").alias("avg_revenue")
)
```

### Unpivot Operations (Melt)
Unpivoting transforms columns into rows. PySpark doesn't have a direct unpivot, but can be achieved with:

#### Stack Function
```python
from pyspark.sql.functions import expr

# Unpivot multiple columns
df.select("id", expr("stack(3, 'col1', col1, 'col2', col2, 'col3', col3) as (metric, value)"))
```

#### Union Approach
```python
# Manual unpivot
df1 = df.select("id", lit("col1").alias("metric"), col("col1").alias("value"))
df2 = df.select("id", lit("col2").alias("metric"), col("col2").alias("value"))
unpivoted = df1.union(df2)
```

---

## Column Renaming

### Single Column Renaming
```python
# Using withColumnRenamed
df.withColumnRenamed("old_name", "new_name")

# Using alias in select
df.select(col("old_name").alias("new_name"))

# Using AS
df.select("old_name AS new_name")
```

### Bulk Column Renaming
```python
# Dictionary-based renaming
rename_dict = {"old1": "new1", "old2": "new2"}
for old, new in rename_dict.items():
    df = df.withColumnRenamed(old, new)

# Using list comprehension with select
new_columns = [col(c).alias(c.replace("old_", "new_")) for c in df.columns]
df.select(*new_columns)

# Function-based renaming
def rename_columns(df, func):
    return df.select(*[col(c).alias(func(c)) for c in df.columns])

# Usage: snake_case to camelCase
df_renamed = rename_columns(df, lambda x: x.replace("_", "").title())
```

### Pattern-Based Renaming
```python
import re

# Remove prefixes
df.select(*[col(c).alias(re.sub(r'^prefix_', '', c)) for c in df.columns])

# Add suffixes
df.select(*[col(c).alias(f"{c}_suffix") for c in df.columns])
```

---

## Anti and Semi Joins

### Semi Join
Returns rows from the left DataFrame that have matching keys in the right DataFrame (but doesn't include right DataFrame columns).

```python
# Semi join - returns left rows with matches in right
result = left_df.join(right_df, "key", "semi")

# Equivalent using exists
result = left_df.filter(col("key").isin(right_df.select("key")))
```

### Anti Join
Returns rows from the left DataFrame that don't have matching keys in the right DataFrame.

```python
# Anti join - returns left rows without matches in right
result = left_df.join(right_df, "key", "anti")

# Equivalent using not exists
right_keys = right_df.select("key").distinct()
result = left_df.filter(~col("key").isin(right_keys))
```

### Use Cases
- **Semi Join**: Finding customers who made purchases
- **Anti Join**: Finding customers who never made purchases
- **Data Validation**: Checking referential integrity

### Performance Considerations
- Semi/anti joins are more efficient than exists/not exists subqueries
- Use broadcast joins for small right DataFrames
- Consider using `distinct()` on join keys for better performance

---

## Handling Large Files

### File Format Considerations
#### Parquet (Recommended)
- **Columnar storage**: Efficient for analytics
- **Compression**: Built-in compression algorithms
- **Schema evolution**: Supports adding/removing columns
- **Predicate pushdown**: Filters applied at file level

#### Delta Lake
- **ACID transactions**: Ensures data consistency
- **Time travel**: Version history and rollback
- **Schema enforcement**: Prevents schema drift
- **Optimizations**: Auto-compaction and Z-ordering

### Reading Strategies
```python
# Partitioned reading
df = spark.read.parquet("path/to/partitioned/data")

# Schema specification (avoids schema inference)
df = spark.read.schema(predefined_schema).parquet("path")

# Predicate pushdown
df = spark.read.parquet("path").filter(col("date") >= "2023-01-01")
```

### Writing Strategies
```python
# Partitioned writing
df.write.partitionBy("year", "month").parquet("output/path")

# Bucketing
df.write.bucketBy(10, "id").sortBy("timestamp").saveAsTable("table_name")

# Coalesce before writing
df.coalesce(1).write.parquet("output/path")
```

### Partitioning Strategies
- **Time-based partitioning**: By date/year/month
- **Hash partitioning**: For even distribution
- **Range partitioning**: For ordered data
- **Avoid over-partitioning**: Too many small files hurt performance

---

## Performance Tuning

### Spark Configuration
```python
# Executor configuration
spark.conf.set("spark.executor.memory", "4g")
spark.conf.set("spark.executor.cores", "4")
spark.conf.set("spark.executor.instances", "10")

# Serialization
spark.conf.set("spark.serializer", "org.apache.spark.serializer.KryoSerializer")

# Adaptive Query Execution
spark.conf.set("spark.sql.adaptive.enabled", "true")
spark.conf.set("spark.sql.adaptive.coalescePartitions.enabled", "true")
```

### Optimization Techniques

#### Caching and Persistence
```python
# Cache frequently accessed DataFrames
df.cache()  # MEMORY_AND_DISK
df.persist(StorageLevel.MEMORY_ONLY)

# Unpersist when done
df.unpersist()
```

#### Broadcast Variables
```python
# Broadcast small DataFrames
from pyspark.sql.functions import broadcast

large_df.join(broadcast(small_df), "key")
```

#### Bucketing
```python
# Pre-partition data for joins
df.write.bucketBy(10, "join_key").saveAsTable("bucketed_table")
```

### Query Optimization
- **Catalyst Optimizer**: Automatic query optimization
- **Predicate Pushdown**: Move filters closer to data source
- **Column Pruning**: Read only required columns
- **Projection Pushdown**: Select only needed columns early

### Monitoring and Debugging
```python
# Execution plan
df.explain(True)

# Query execution time
df.count()  # Triggers computation
```

---

## Handling OOM Issues

### Common Causes
1. **Skewed data**: Uneven partition distribution
2. **Large broadcast variables**: Broadcasting huge DataFrames
3. **Inefficient joins**: Cartesian products or large shuffles
4. **Memory leaks**: Not unpersisting cached DataFrames
5. **Insufficient executor memory**: Poor memory configuration

### Prevention Strategies

#### Memory Configuration
```python
# Increase executor memory
spark.conf.set("spark.executor.memory", "8g")
spark.conf.set("spark.executor.memoryFraction", "0.8")

# Increase driver memory for collect operations
spark.conf.set("spark.driver.memory", "4g")
```

#### Data Skew Handling
```python
# Salting for skewed joins
from pyspark.sql.functions import rand, floor

# Add salt to skewed key
df_salted = df.withColumn("salted_key", 
                         concat(col("skewed_key"), lit("_"), 
                               floor(rand() * 100).cast("string")))
```

#### Partition Management
```python
# Increase partitions for large datasets
df.repartition(1000)

# Coalesce to reduce partitions
df.coalesce(100)

# Check partition count
df.rdd.getNumPartitions()
```

### Memory Management Best Practices

#### Efficient Data Types
```python
# Use smaller data types
df.withColumn("small_int", col("big_int").cast("int"))

# String interning for repeated values
df.withColumn("category", when(col("category").isNull(), "unknown")
              .otherwise(col("category")))
```

#### Streaming Processing
```python
# Process large datasets in chunks
def process_partition(iterator):
    for partition in iterator:
        # Process partition
        yield processed_partition

df.mapPartitions(process_partition)
```

#### Garbage Collection Tuning
```python
# G1 garbage collector
spark.conf.set("spark.executor.extraJavaOptions", 
               "-XX:+UseG1GC -XX:MaxGCPauseMillis=200")
```

### Recovery Strategies
1. **Increase parallelism**: More partitions, smaller tasks
2. **Reduce batch size**: Process smaller chunks
3. **Use iterative processing**: Break complex operations into steps
4. **Implement checkpointing**: Save intermediate results
5. **Monitor resource usage**: Use Spark UI for debugging

### Monitoring OOM Issues
```python
# Check DataFrame size
df.rdd.map(lambda x: len(str(x))).sum()

# Monitor partition sizes
df.rdd.mapPartitions(lambda iterator: [sum(1 for _ in iterator)]).collect()

# Use explain to understand execution plan
df.explain(True)
```

---

## Best Practices Summary

1. **Choose appropriate file formats**: Parquet for analytics, Delta for transactional workloads
2. **Optimize partitioning**: Balance between too few and too many partitions
3. **Use proper join strategies**: Broadcast small tables, bucket large tables
4. **Monitor and tune**: Regular performance monitoring and configuration tuning
5. **Handle data skew**: Use salting, custom partitioning, or bucketing
6. **Manage memory**: Proper executor sizing and garbage collection tuning
7. **Cache wisely**: Cache frequently accessed DataFrames, unpersist when done
8. **Test with realistic data**: Performance characteristics change with data size

Remember: PySpark's power comes from its distributed computing model, but this also introduces complexity in optimization and debugging. Understanding the underlying concepts and monitoring tools is crucial for effective development.